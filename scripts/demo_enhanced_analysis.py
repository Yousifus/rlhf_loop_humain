#!/usr/bin/env python3
"""
üöÄ Enhanced Analysis Demo for RLHF System

This script demonstrates the new Phase 1 enhanced analysis capabilities:
- Advanced Calibration Analysis with MCE, ACE, KL metrics
- Enhanced Drift Detection with PSI and statistical tests  
- Real-time Monitoring with alerts and predictions

Usage: python scripts/demo_enhanced_analysis.py
"""

import sys
import os
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from pathlib import Path
from datetime import datetime, timedelta
import time

# Add project root to path
project_root = str(Path(__file__).resolve().parents[1])
if project_root not in sys.path:
    sys.path.append(project_root)

try:
    # Import our new enhanced analysis modules
    from utils.analysis.calibration_enhanced import (
        AdvancedCalibrationAnalyzer, 
        CalibrationMetrics,
        calculate_isotonic_calibration
    )
    from utils.analysis.drift_enhanced import (
        EnhancedDriftDetector,
        calculate_psi,
        statistical_drift_test,
        feature_drift_analysis
    )
    from utils.analysis.real_time_monitor import (
        RealTimeMonitor,
        MonitoringConfig,
        Alert
    )
except ImportError as e:
    print(f"‚ö†Ô∏è  Import warning: {e}")
    print("Some enhanced analysis features may not be available.")
    print("Make sure all required packages are installed: numpy, pandas, plotly, scipy, sklearn")

def generate_demo_data():
    """Generate realistic demo data for analysis."""
    print("üìä Generating demo data...")
    
    np.random.seed(42)
    n_samples = 1000
    
    # Generate base dataset (reference)
    reference_accuracy = np.random.beta(8, 2, n_samples)  # Skewed towards high accuracy
    reference_confidence = reference_accuracy + np.random.normal(0, 0.1, n_samples)
    reference_confidence = np.clip(reference_confidence, 0, 1)
    
    # True labels based on accuracy
    reference_y_true = (np.random.random(n_samples) < reference_accuracy).astype(int)
    
    # Create reference DataFrame
    reference_df = pd.DataFrame({
        'confidence': reference_confidence,
        'accuracy': reference_accuracy,
        'y_true': reference_y_true,
        'timestamp': pd.date_range(start='2024-01-01', periods=n_samples, freq='1H'),
        'feature_1': np.random.normal(0, 1, n_samples),
        'feature_2': np.random.exponential(1, n_samples),
        'feature_3': np.random.choice(['A', 'B', 'C'], n_samples)
    })
    
    # Generate current dataset with some drift
    drift_factor = 0.3
    current_accuracy = reference_accuracy - drift_factor * np.random.beta(2, 8, n_samples)
    current_confidence = reference_confidence + np.random.normal(0.1, 0.05, n_samples)  # Overconfidence
    current_confidence = np.clip(current_confidence, 0, 1)
    
    current_y_true = (np.random.random(n_samples) < current_accuracy).astype(int)
    
    # Add drift to features
    current_df = pd.DataFrame({
        'confidence': current_confidence,
        'accuracy': current_accuracy,
        'y_true': current_y_true,
        'timestamp': pd.date_range(start='2024-06-01', periods=n_samples, freq='1H'),
        'feature_1': np.random.normal(0.5, 1.2, n_samples),  # Mean shift + variance change
        'feature_2': np.random.exponential(1.5, n_samples),  # Scale change
        'feature_3': np.random.choice(['A', 'B', 'C', 'D'], n_samples, p=[0.4, 0.3, 0.2, 0.1])  # New category
    })
    
    print(f"‚úÖ Generated {n_samples} reference samples and {n_samples} current samples")
    return reference_df, current_df

def demo_advanced_calibration(reference_df, current_df):
    """Demonstrate advanced calibration analysis."""
    print("\nüéØ === ADVANCED CALIBRATION ANALYSIS ===")
    
    try:
        # Initialize analyzer
        analyzer = AdvancedCalibrationAnalyzer(random_state=42)
        
        # Analyze reference data
        print("üìà Analyzing reference dataset calibration...")
        ref_metrics = analyzer.calculate_all_metrics(
            y_true=reference_df['y_true'].values,
            y_prob=reference_df['confidence'].values,
            n_bins=10,
            n_bootstrap=100  # Reduced for demo speed
        )
        
        print(f"   ECE: {ref_metrics.ece:.4f}")
        print(f"   MCE: {ref_metrics.mce:.4f}")
        print(f"   ACE: {ref_metrics.ace:.4f}")
        print(f"   KL Calibration: {ref_metrics.kl_calibration:.4f}")
        print(f"   Brier Score: {ref_metrics.brier_score:.4f}")
        
        # Analyze current data
        print("üìâ Analyzing current dataset calibration...")
        curr_metrics = analyzer.calculate_all_metrics(
            y_true=current_df['y_true'].values,
            y_prob=current_df['confidence'].values,
            n_bins=10,
            n_bootstrap=100
        )
        
        print(f"   ECE: {curr_metrics.ece:.4f}")
        print(f"   MCE: {curr_metrics.mce:.4f}")
        print(f"   ACE: {curr_metrics.ace:.4f}")
        print(f"   KL Calibration: {curr_metrics.kl_calibration:.4f}")
        print(f"   Brier Score: {curr_metrics.brier_score:.4f}")
        
        # Compare improvements
        print("\nüìä Calibration Changes:")
        print(f"   ECE Change: {curr_metrics.ece - ref_metrics.ece:+.4f}")
        print(f"   MCE Change: {curr_metrics.mce - ref_metrics.mce:+.4f}")
        print(f"   Brier Change: {curr_metrics.brier_score - ref_metrics.brier_score:+.4f}")
        
        # Demonstrate isotonic calibration
        print("\nüîß Applying isotonic calibration correction...")
        calibrated_probs, regressor = calculate_isotonic_calibration(
            reference_df['y_true'].values,
            reference_df['confidence'].values
        )
        
        # Calculate metrics after calibration
        corrected_metrics = analyzer.calculate_all_metrics(
            y_true=reference_df['y_true'].values,
            y_prob=calibrated_probs,
            n_bins=10,
            n_bootstrap=50
        )
        
        print(f"   Original ECE: {ref_metrics.ece:.4f}")
        print(f"   Corrected ECE: {corrected_metrics.ece:.4f}")
        print(f"   Improvement: {ref_metrics.ece - corrected_metrics.ece:.4f}")
        
        # Create enhanced reliability plot
        print("üìä Creating enhanced reliability diagram...")
        fig = analyzer.create_enhanced_reliability_plot(ref_metrics, "Reference Data Reliability")
        
        # Save plot
        output_dir = Path("demo_outputs")
        output_dir.mkdir(exist_ok=True)
        fig.write_html(str(output_dir / "enhanced_reliability_diagram.html"))
        print(f"   Saved to: {output_dir / 'enhanced_reliability_diagram.html'}")
        
        return ref_metrics, curr_metrics, corrected_metrics
        
    except Exception as e:
        print(f"‚ùå Error in calibration analysis: {e}")
        return None, None, None

def demo_enhanced_drift_detection(reference_df, current_df):
    """Demonstrate enhanced drift detection."""
    print("\nüîÑ === ENHANCED DRIFT DETECTION ===")
    
    try:
        # Initialize drift detector
        detector = EnhancedDriftDetector(
            psi_thresholds={
                'no_drift': 0.1,
                'slight_drift': 0.2,
                'moderate_drift': 0.3,
                'severe_drift': float('inf')
            },
            significance_level=0.05
        )
        
        # Fit reference data
        print("üìä Fitting reference dataset...")
        feature_cols = ['feature_1', 'feature_2', 'confidence']
        detector.fit_reference(reference_df, feature_cols)
        
        # Analyze drift
        print("üîç Analyzing drift in current dataset...")
        drift_report = detector.analyze_drift(current_df, detailed_analysis=True)
        
        print(f"\nüìà Drift Analysis Results:")
        print(f"   Overall Drift Detected: {drift_report.overall_drift_detected}")
        print(f"   Drift Severity: {drift_report.drift_severity}")
        print(f"   Confidence Score: {drift_report.confidence_score:.3f}")
        
        print(f"\nüéØ PSI Results:")
        for psi_result in drift_report.psi_results:
            print(f"   {psi_result.feature_name}:")
            print(f"     PSI Score: {psi_result.psi_score:.4f}")
            print(f"     Drift Level: {psi_result.drift_level}")
        
        print(f"\nüìä Statistical Test Results:")
        for test_result in drift_report.statistical_tests:
            print(f"   {test_result.test_name}:")
            print(f"     Statistic: {test_result.statistic:.4f}")
            print(f"     P-value: {test_result.p_value:.4f}")
            print(f"     Significant: {test_result.is_significant}")
        
        print(f"\nüí° Recommendations:")
        for rec in drift_report.recommendations:
            print(f"   ‚Ä¢ {rec}")
        
        # Create drift dashboard
        print("\nüìä Creating drift analysis dashboard...")
        drift_fig = detector.create_drift_dashboard(drift_report)
        
        # Save dashboard
        output_dir = Path("demo_outputs")
        drift_fig.write_html(str(output_dir / "drift_analysis_dashboard.html"))
        print(f"   Saved to: {output_dir / 'drift_analysis_dashboard.html'}")
        
        # Demo individual PSI calculation
        print("\nüî¢ Individual PSI Calculation Demo:")
        psi_score = calculate_psi(
            reference_df['confidence'], 
            current_df['confidence']
        )
        print(f"   Confidence PSI: {psi_score:.4f}")
        
        return drift_report
        
    except Exception as e:
        print(f"‚ùå Error in drift detection: {e}")
        return None

def demo_real_time_monitoring(reference_df, current_df):
    """Demonstrate real-time monitoring capabilities."""
    print("\n‚ö° === REAL-TIME MONITORING ===")
    
    try:
        # Setup monitoring configuration
        def alert_callback(alert):
            print(f"üö® ALERT: {alert.severity.upper()} - {alert.message}")
        
        config = MonitoringConfig(
            calibration_drift_threshold=0.03,
            performance_drop_threshold=0.05,
            alert_cooldown_minutes=1,  # Short for demo
            metrics_window_size=50,
            enable_auto_alerts=True,
            alert_callbacks=[alert_callback]
        )
        
        # Initialize monitor
        monitor = RealTimeMonitor(config)
        
        print("üìä Starting real-time monitoring simulation...")
        print("   (Processing reference data to establish baseline)")
        
        # Process reference data to establish baseline
        for _, row in reference_df.head(100).iterrows():
            monitor.add_observation(
                y_true=int(row['y_true']),
                y_prob=float(row['confidence']),
                features={'feature_1': row['feature_1']}
            )
        
        # Check status after baseline
        status = monitor.get_current_status()
        print(f"   Baseline established: {status['baseline_set']}")
        print(f"   Buffer size: {status['buffer_size']}")
        
        # Process some current data to trigger alerts
        print("\n‚ö†Ô∏è  Processing current data (may trigger alerts)...")
        for _, row in current_df.head(50).iterrows():
            monitor.add_observation(
                y_true=int(row['y_true']),
                y_prob=float(row['confidence']),
                features={'feature_1': row['feature_1']}
            )
            time.sleep(0.01)  # Small delay to simulate real-time
        
        # Get final status
        final_status = monitor.get_current_status()
        print(f"\nüìà Final Monitoring Status:")
        print(f"   Latest Accuracy: {final_status['latest_metrics']['accuracy']:.4f}")
        print(f"   Latest Calibration Error: {final_status['latest_metrics']['calibration_error']:.4f}")
        print(f"   Accuracy Trend: {final_status['trends']['accuracy']}")
        print(f"   Calibration Trend: {final_status['trends']['calibration']}")
        print(f"   Active Alerts: {final_status['active_alerts']}")
        
        # Get alert summary
        alert_summary = monitor.alert_manager.get_alert_summary()
        print(f"\nüö® Alert Summary:")
        print(f"   Total Active Alerts: {alert_summary['total_active']}")
        print(f"   Total All-Time Alerts: {alert_summary['total_all_time']}")
        print(f"   By Severity: {alert_summary['by_severity']}")
        print(f"   By Type: {alert_summary['by_type']}")
        
        # Get predictions if available
        predictions = monitor.get_predictions()
        if predictions['status'] == 'predictions_available':
            print(f"\nüîÆ Performance Predictions:")
            pred_data = predictions['predictions']
            print(f"   Health Score: {pred_data['health_score']:.3f}")
            print(f"   Risk Level: {pred_data['risk_assessment']['overall_risk']}")
            print(f"   Accuracy Trend: {pred_data['accuracy']['trend']}")
            print(f"   Calibration Trend: {pred_data['calibration_error']['trend']}")
        else:
            print(f"\nüîÆ Predictions: {predictions['status']}")
        
        return monitor, final_status, alert_summary
        
    except Exception as e:
        print(f"‚ùå Error in real-time monitoring: {e}")
        return None, None, None

def create_summary_report(calibration_results, drift_results, monitoring_results):
    """Create a comprehensive summary report."""
    print("\nüìã === COMPREHENSIVE ANALYSIS SUMMARY ===")
    
    try:
        ref_metrics, curr_metrics, corrected_metrics = calibration_results
        drift_report = drift_results
        monitor, final_status, alert_summary = monitoring_results
        
        print("üéØ CALIBRATION ANALYSIS:")
        if ref_metrics and curr_metrics:
            print(f"   ‚Ä¢ ECE degradation: {curr_metrics.ece - ref_metrics.ece:+.4f}")
            print(f"   ‚Ä¢ MCE degradation: {curr_metrics.mce - ref_metrics.mce:+.4f}")
            if corrected_metrics:
                print(f"   ‚Ä¢ Isotonic correction improved ECE by: {ref_metrics.ece - corrected_metrics.ece:.4f}")
        
        print("\nüîÑ DRIFT DETECTION:")
        if drift_report:
            print(f"   ‚Ä¢ Overall drift detected: {drift_report.overall_drift_detected}")
            print(f"   ‚Ä¢ Severity level: {drift_report.drift_severity}")
            severe_features = [psi.feature_name for psi in drift_report.psi_results if psi.drift_level == 'severe_drift']
            if severe_features:
                print(f"   ‚Ä¢ Features with severe drift: {', '.join(severe_features)}")
        
        print("\n‚ö° REAL-TIME MONITORING:")
        if final_status and alert_summary:
            print(f"   ‚Ä¢ Active alerts: {alert_summary['total_active']}")
            print(f"   ‚Ä¢ Accuracy trend: {final_status['trends']['accuracy']}")
            print(f"   ‚Ä¢ Calibration trend: {final_status['trends']['calibration']}")
        
        print("\nüí° RECOMMENDATIONS:")
        print("   ‚Ä¢ ‚úÖ Enhanced calibration analysis is working")
        print("   ‚Ä¢ ‚úÖ Advanced drift detection is operational")
        print("   ‚Ä¢ ‚úÖ Real-time monitoring is functional")
        print("   ‚Ä¢ üéØ Ready for Phase 2 implementation!")
        
    except Exception as e:
        print(f"‚ùå Error creating summary: {e}")

def main():
    """Main demo function."""
    print("üöÄ RLHF Enhanced Analysis Demo - Phase 1 Implementation")
    print("=" * 60)
    
    # Generate demo data
    reference_df, current_df = generate_demo_data()
    
    # Demo each component
    calibration_results = demo_advanced_calibration(reference_df, current_df)
    drift_results = demo_enhanced_drift_detection(reference_df, current_df)
    monitoring_results = demo_real_time_monitoring(reference_df, current_df)
    
    # Create summary
    create_summary_report(calibration_results, drift_results, monitoring_results)
    
    print("\nüéâ Enhanced Analysis Demo Complete!")
    print("üìÅ Check 'demo_outputs' directory for generated visualizations")
    print("üî• Phase 1 enhancements are ready for production use!")

if __name__ == "__main__":
    main()